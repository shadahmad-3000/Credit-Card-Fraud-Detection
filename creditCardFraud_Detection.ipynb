{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score , precision_score , confusion_matrix , classification_report\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset to a Pandas DataFrame\n",
    "credit_card_data = pd.read_csv('creditcard.csv')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 rows of the dataset\n",
    "print('THE FIRST FIVE ROWS-')\n",
    "credit_card_data.head()\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THE LAST FIVE ROWS-')\n",
    "credit_card_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset informations\n",
    "print('DATASET INFORMATION')\n",
    "credit_card_data.info()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of missing values in each column\n",
    "credit_card_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the above data \n",
    "print('COUNT-PLOT FOR LEGIT AND FRAUD TRANSACTIONS-')\n",
    "\n",
    "sns.countplot(x='Class',data=credit_card_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of legit transactions & fraudulent transactions\n",
    "credit_card_data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the data for analysis\n",
    "legit = credit_card_data[credit_card_data.Class == 0]\n",
    "fraud = credit_card_data[credit_card_data.Class == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SHAPE OF LEGIT AND FRAUD-')\n",
    "print(legit.shape)\n",
    "print(fraud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical measures of the data\n",
    "print('DESCRIPTION OF AMOUNT ON BASIS OF LEGIT TRANSACTION-')\n",
    "legit.Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DESCRIPTION OF AMOUNT ON BASIS OF FRAUD TRANSACTION-')\n",
    "fraud.Amount.describe()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compare the values for both transactions\n",
    "credit_card_data.groupby('Class').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#under-sampling the data\n",
    "#HANDLING IMBALANCE DATASET\n",
    "legit_sample = legit.sample(n=492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.concat([legit_sample, fraud], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the above data \n",
    "print('COUNT-PLOT FOR NEW DATASET-')\n",
    "sns.countplot(x='Class',data=new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first five rows of new dataset\n",
    "print(' FIRST FIVE ROWS OF NEW DATASET-')\n",
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LAST FIVE ROWS OF NEW DATASET-')\n",
    "new_dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.groupby('Class').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "print('CORRELATION MATRIX-')\n",
    "corrmat = new_dataset.corr()\n",
    "fig = plt.figure(figsize = (12, 9))\n",
    "sns.heatmap(corrmat, vmax = .8, square = True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating features and targets\n",
    "\n",
    "X = new_dataset.drop(columns='Class', axis=1)\n",
    "Y = new_dataset['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FEATURES-',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('LABELS-',Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPILITTING THE NEW DATASET -\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)\n",
    "scaler=StandardScaler()\n",
    "x_train=scaler.fit_transform(X_train)\n",
    "x_test=scaler.fit_transform(X_test)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the Logistic Regression Model with Training Data\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, Y_train)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy on training data\n",
    "X_train_prediction = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on Training data : ', training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy on test data\n",
    "X_test_prediction = model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy score on Test Data : ', test_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec= precision_score(X_test_prediction, Y_test)\n",
    "print(\"The precision is {}\".format(prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "class_reports=classification_report(Y_test,X_test_prediction)\n",
    "print(\"CLASS REPORT IS \",class_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the confusion matrix # TP TN FP FN\n",
    "LABELS = ['Legit', 'Fraud']\n",
    "conf_matrix = confusion_matrix(Y_test, X_test_prediction)\n",
    "plt.figure(figsize =(12, 12))\n",
    "sns.heatmap(conf_matrix, xticklabels = LABELS, \n",
    "            yticklabels = LABELS, annot = True, fmt =\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect user input\n",
    "def collect_transaction_data():\n",
    "    transaction_data = {}\n",
    "    for column in X.columns:\n",
    "        transaction_data[column] = float(input(f\"Enter value for {column}: \"))\n",
    "    return transaction_data\n",
    "\n",
    "# Function to predict fraud\n",
    "def predict_fraud(transaction_data, model):\n",
    "    transaction_df = pd.DataFrame([transaction_data])\n",
    "    prediction = model.predict(transaction_df)\n",
    "    return \"Fraudulent\" if prediction[0] == 1 else \"Legitimate\"\n",
    "\n",
    "# Example usage\n",
    "transaction_data = collect_transaction_data()\n",
    "fraud_prediction = predict_fraud(transaction_data, model)\n",
    "print('DETAILS OF TRANSACTION-',transaction_data)\n",
    "print(f\"This transaction is potentially {fraud_prediction}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
